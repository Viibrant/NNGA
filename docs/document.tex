\documentclass[conference]{inc/IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\include{inc/packages.inc}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Neural Network-Based Iris Classification and Genetic Algorithm-Driven SHUBERT Function Optimisation \\
{\footnotesize Completed as part of \textit{M25352 Neural Networks and Genetic Algorithms}}
}


\author{\IEEEauthorblockN{Connor Brook}
    \IEEEauthorblockA{\textit{BSc. Data Science and Analytics} \\
        \textit{University of Portsmouth}\\
        Portsmouth, United Kingdom \\
        brook@connordata.science}
    \and
    \IEEEauthorblockN{Jamie Doe}
    \IEEEauthorblockA{\textit{BSc. Software Engineering} \\
        \textit{University of Portsmouth}\\
        Portsmouth, United Kingdom \\
        UP953068@myport.ac.uk}
}

\maketitle

\begin{abstract}
    In this paper, we explore two different optimisation problems: a neural network for the Iris dataset and a genetic algorithm for the SHUBERT function. We design, implement, and evaluate both optimisation techniques, comparing their performance and discussing the advantages and disadvantages of each method. The artificial neural network is employed to classify iris flowers based on their morphological features, while the genetic algorithm is utilized to optimize the SHUBERT function, a well-known benchmark problem in the global optimisation domain. By comparing the results of these two distinct optimisation approaches, we provide insights into their applicability and effectiveness in solving different types of problems.
\end{abstract}

\begin{IEEEkeywords}
    component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
In recent years, optimisation techniques have gained significant attention due to their wide range of applications in various fields such as machine learning, engineering, and finance. Among these techniques, artificial neural networks (ANNs) and genetic algorithms (GAs) have emerged as powerful tools for solving complex problems. This paper explores the application of ANNs to classify the Iris dataset, a well-known benchmark problem in machine learning, and the use of GAs to optimize the SHUBERT function, a challenging global optimisation problem. By comparing the performance of these two distinct optimisation methods, we aim to provide a deeper understanding of their strengths and weaknesses, as well as their suitability for different types of problems. This paper will cover the design, implementation, evaluation, and comparison of ANNs and GAs in the context of the Iris dataset classification and SHUBERT function optimization.

\section{Part I: Neural Networks for Classification/Mapping Task}

\subsection{Introduction}

The Iris dataset is a classic machine learning classification problem, introduced in 1936 by Ronald A.
Fisher. This report aims to create an artificial neural network to accurately classify Iris plants into
three species: Iris Setosa, Iris Versicolour, and Iris Virginica. We tackle the challenge of non-linear
separability between two species through data preprocessing, neural network design, optimization,validation,
and evaluation. Our goal is to provide insights and pave the way for future research in this field.

\subsection{Data Analysis and Pre-processing}

\subsubsection{Data Analysis}
Before the ANN's architecture can be theorised, we first must perform a comprehensive data analysis of the
Iris dataset using various visualization techniques. This will allow us to gain a deeper understanding of the
relationships between the attributes and the distribution of the different Iris species.

\subsubsection{Data Pre-processing}



\subsection{Neural Network Design}

\subsubsection{Topology and Architecture}

\subsubsection{Training Algorithm (Backpropagation)}

\subsection{Training, Validation, and Testing}

\subsubsection{Split-sample}

\subsubsection{Cross-validation}

\subsection{Results and Post-Processing}

\section{Part II: Genetic Algorithms for Global Optimization Problem}

\subsection{Function Selection and Analysis}
\subsection{Genetic Algorithm Design}

\subsubsection{Representation of Initial Population}

\subsubsection{Fitness Function}

\subsubsection{Selection Method}

\subsubsection{Reproductive Operators}

\subsubsection{Stopping Criteria}

\subsubsection{Parameters and Constraints}
\subsection{GA Analysis of the Run}

\subsubsection{Selection Strategy}

\subsubsection{Comparison of Strategies}
\subsection{Results and Discussion}

\subsubsection{Premature Convergence}

\subsubsection{Maintaining Selection Pressure}

\subsubsection{Balancing Exploration and Exploitation}

\section{Conclusion}

\section*{References}


\bibliography{bib/IEEEabrv, bib/mybib}
\bibliographystyle{inc/IEEEtranS}


\end{document}